{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a762f60-708c-45fd-a243-d7a653131493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "                                              Review  label\n",
      "0  Amber and LaDonna at the Starbucks on Southwes...      1\n",
      "1  ** at the Starbucks by the fire station on 436...      1\n",
      "2  I just wanted to go out of my way to recognize...      1\n",
      "3  Me and my friend were at Starbucks and my card...      1\n",
      "4  I’m on this kick of drinking 5 cups of warm wa...      1\n",
      "Class distribution:\n",
      "label\n",
      "0    583\n",
      "1    122\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/20, loss = 5.4995\n",
      "Epoch 2/20, loss = 4.2335\n",
      "Epoch 3/20, loss = 4.0829\n",
      "Epoch 4/20, loss = 4.0479\n",
      "Epoch 5/20, loss = 3.9752\n",
      "Epoch 6/20, loss = 3.9580\n",
      "Epoch 7/20, loss = 3.9600\n",
      "Epoch 8/20, loss = 3.8531\n",
      "Epoch 9/20, loss = 3.7999\n",
      "Epoch 10/20, loss = 3.6955\n",
      "Epoch 11/20, loss = 3.6113\n",
      "Epoch 12/20, loss = 3.6159\n",
      "Epoch 13/20, loss = 3.3657\n",
      "Epoch 14/20, loss = 3.2385\n",
      "Epoch 15/20, loss = 3.1271\n",
      "Epoch 16/20, loss = 2.9618\n",
      "Epoch 17/20, loss = 2.7284\n",
      "Epoch 18/20, loss = 2.6504\n",
      "Epoch 19/20, loss = 2.4564\n",
      "Epoch 20/20, loss = 2.1709\n",
      "Epoch 1/20, loss = 5.1729\n",
      "Epoch 2/20, loss = 4.2186\n",
      "Epoch 3/20, loss = 4.1478\n",
      "Epoch 4/20, loss = 4.1278\n",
      "Epoch 5/20, loss = 4.1096\n",
      "Epoch 6/20, loss = 4.0859\n",
      "Epoch 7/20, loss = 4.1182\n",
      "Epoch 8/20, loss = 4.0284\n",
      "Epoch 9/20, loss = 4.0180\n",
      "Epoch 10/20, loss = 3.9830\n",
      "Epoch 11/20, loss = 3.9095\n",
      "Epoch 12/20, loss = 3.8896\n",
      "Epoch 13/20, loss = 3.8134\n",
      "Epoch 14/20, loss = 3.7385\n",
      "Epoch 15/20, loss = 3.7247\n",
      "Epoch 16/20, loss = 3.6210\n",
      "Epoch 17/20, loss = 3.6219\n",
      "Epoch 18/20, loss = 3.5851\n",
      "Epoch 19/20, loss = 3.4926\n",
      "Epoch 20/20, loss = 3.4575\n",
      "LSTM: 0.7730\n",
      "GloVe: 0.8298\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import re, string\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "df = pd.read_csv(\"reviews_data.csv\")#https://www.kaggle.com/datasets/harshalhonde/starbucks-reviews-dataset\n",
    "\n",
    "TEXT_COL = \"Review\"\n",
    "RATING_COL = \"Rating\"\n",
    "\n",
    "df = df.dropna(subset=[TEXT_COL, RATING_COL])\n",
    "\n",
    "df[\"label\"] = (df[RATING_COL] >= 4).astype(int)\n",
    "\n",
    "print(df[[TEXT_COL, \"label\"]].head())\n",
    "print(\"Class distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()                                       # до нижнього регістру\n",
    "    text = re.sub(r\"@\\S+\", \" \", text)                         # прибрати згадки @user\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)                      # прибрати посилання\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)                        # прибрати HTML-теги\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)                     # залишити лише букви (прибрати цифри, спецсимволи)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # прибрати пунктуацію\n",
    "    text = re.sub(r\"\\s+\", \" \", text)                          # замінити багато пробілів на один\n",
    "    return text\n",
    "\n",
    "tokenized_texts = df[TEXT_COL].apply(tokenize)\n",
    "\n",
    "\n",
    "MAX_VOCAB = 20000\n",
    "counter = Counter()\n",
    "\n",
    "for tokens in tokenized_texts:\n",
    "    counter.update(tokens)\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word, _ in counter.most_common(MAX_VOCAB - 2):\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "def encode(tokens):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
    "\n",
    "encoded_texts = tokenized_texts.apply(encode)\n",
    "\n",
    "MAX_LEN = 100\n",
    "\n",
    "def pad_sequence(seq):\n",
    "    if len(seq) < MAX_LEN:\n",
    "        return seq + [0] * (MAX_LEN - len(seq))\n",
    "    return seq[:MAX_LEN]\n",
    "\n",
    "X = torch.tensor(\n",
    "    [pad_sequence(seq) for seq in encoded_texts],\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "y = torch.tensor(df[\"label\"].values, dtype=torch.float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader  = DataLoader(TextDataset(X_test, y_test), batch_size=64)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(emb)\n",
    "        out = self.fc(h[-1])\n",
    "        return out.squeeze(1)\n",
    "\n",
    "def train_and_evaluate(model, epochs=20):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, loss = {total_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds.extend((probs > 0.5).cpu().numpy())\n",
    "            true.extend(yb.numpy())\n",
    "\n",
    "    return accuracy_score(true, preds)\n",
    "\n",
    "model_A = LSTM(len(vocab))\n",
    "acc_A = train_and_evaluate(model_A)\n",
    "\n",
    "\n",
    "def load_glove_embeddings(path, vocab, emb_dim=100):\n",
    "    embeddings = np.random.normal(scale=0.6, size=(len(vocab), emb_dim))\n",
    "\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            word = parts[0]\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = np.array(parts[1:], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "glove_weights = load_glove_embeddings(\"glove.6B.100d.txt\", vocab)\n",
    "\n",
    "class LSTM_GloVe(nn.Module):\n",
    "    def __init__(self, emb_weights, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            emb_weights, freeze=False\n",
    "        )\n",
    "        self.lstm = nn.LSTM(emb_weights.size(1), hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(emb)\n",
    "        return self.fc(h[-1]).squeeze(1)\n",
    "\n",
    "model_B = LSTM_GloVe(glove_weights)\n",
    "acc_B = train_and_evaluate(model_B)\n",
    "\n",
    "\n",
    "print(f\"LSTM: {acc_A:.4f}\")\n",
    "print(f\"GloVe: {acc_B:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d6e64-b9e5-4d8a-b3d2-78406acfe17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
